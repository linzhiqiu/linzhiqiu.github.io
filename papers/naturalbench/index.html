<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="">
  <meta property="og:title" content="NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples" />
  <meta property="og:description" content="" />
  <meta property="og:url" content="" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples">
  <meta name="twitter:description" content="">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.31.2/gradio.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span style="color: green;">[NeurIPS 2024 D&B]</span></h1>
            <h1 class="title is-1 publication-title">NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a target="_blank">Baiqi Li</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a target="_blank">Zhiqiu Lin</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a target="_blank">Wenxuan Peng</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a target="_blank">Jean de Dieu Nyandwi</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a target="_blank">Daniel Jiang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a target="_blank">Zixian Ma</a><sup>2</sup>,</span>
              <span class="author-block">
                <a target="_blank">Simran Khanuja</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a target="_blank">Ranjay Krishna</a><sup>&2</sup>,</span>
                  <span class="author-block">
                    <a target="_blank">Graham Neubig</a><sup>&1</sup>,</span>
                    <span class="author-block">
                      <a target="_blank">Deva Ramanan</a><sup>&1</sup>,</span>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Carnegie Mellon University<br></span>,
              <span class="author-block"><sup>2</sup>University of Washington<br></span>,
              <span class="author-block"><sup>*</sup>Co-first authors<br></span>,
              <span class="author-block"><sup>&</sup>Co-senior authors<br></span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.14669" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/Baiqi-Li/NaturalBench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/BaiqiL/NaturalBench/blob/main/README.md" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>NaturalBench</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  
  
  <!-- <gradio-app src="https://zhiqiulin-vqascore.hf.space"></gradio-app> -->
  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Are large vision-language models (VLMs) truly effective? In this work, we show that popular VLMs still struggle with questions about natural images that humans can easily answer, which we term <b>natural adversarial samples</b>. Unlike previous VQA benchmarks such as MME that can be addressed by blind QA models, NaturalBench avoids such shortcuts by pairing each question with two images that yield different answers. We use a simple procedure to collect challenging VQA samples from natural image-text corpora (e.g., Flickr and DOCCI) using foundation models like CLIP and ChatGPT. We collect a new vision-centric VQA benchmark, <b>NaturalBench</b>, for reliably evaluating VLMs with 10,000 human-verified VQA samples. We note several interesting findings:
              <ol type="1">
                <li><b>NaturalBench is hard</b>. <span style="font-size: 95%;">We evaluate 55 popular VLMs including BLIP-3, mPLUG-Owl2, InternLM-XC2, LLaVA-OneVision, Llama3.2, InternVL2, Cambrain-1, Qwen2-VL, and Molmo. Most of them only achieve 1%-20% above random chance performance. Even the best (closed-source) GPT-o3 and GPT-4o lags 40%-50% behind human performance (which is above 90%). </span></li>
                <li><b>NaturalBench is compositional</b>. <span style="font-size: 95%;">NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. Unlike previous work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for more fine-grained evaluation. </li>
                <li><b>NaturalBench exposes significant biases in VLMs</b>. <span style="font-size: 95%;">Most VLMs choose the same answer regardless of the input image (or question). We show that debiasing can be crucial for better performance.  </span></li>
              </ol>
            </p>
          </div>
          <!-- <h2 class="title is-3">Try ranking images with VQAScore!</h2> -->
          <!-- <gradio-app src="https://zhiqiulin-vqascore.hf.space"></gradio-app> -->
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->
  
  <!-- Method Overview -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Why NaturalBench?</h2>
          <div class="content has-text-justified">

            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              There are already many challenging visual-question-answering (VQA) benchmarks like MMMU, ScienceQA, and MMBench, so why NaturalBench? It turns out that these popular VQA benchmarks aren't as "<i>visual</i>" as they seem. For instance, many questions can be answered using <b>commonsense priors</b>, without relying on the image. For instance, ScienceQA-IMG asks "<i>What is the capital of Massachusetts?</i>", which is easily answered as "<i>Boston</i>" by a blind ChatGPT. Below, we highlight several such examples from six previous benchmarks (MME, MMBench, ScienceQA, MMMU, MMStar, AI2D) that can be solved without looking at the image:
            </p>

            <div class="item">
              <!-- Your image here -->
              <div style="display: flex; justify-content: center; align-items: center;">
                <img src="images/bias_more.jpg" alt="Image illustrating blind priors">
            </div>

              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                What's more, even carefully constructed benchmarks like MME and MMStar can suffer from <b>imbalanced answers</b>. For example, MME's question "<i>Does this artwork exist in the form of a painting?</i>" is answered "Yes" 97.5% of the time! We show that finetuning a "blind" GPT-3.5 — using only text and no images — on a random half of each benchmark allows it to significantly outperform random chance (see the <span style="color: red;">red dotted line</span>) on the other half. In many cases, blind GPT-3.5 even matches or surpasses LLaVA-1.5 finetuned on the same data but with images!
              </p>
              
              <div style="display: flex; justify-content: center; align-items: center;">
                <img src="images/blind_finetune.png" alt="Image illustrating finetuning results">
            </div>
            
            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              These issues drive us to create NaturalBench, a vision-centric VQA benchmark immune to blind solutions.
            </p>

              
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Method Overview -->
  

  <!-- Method Overview -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Collecting a Vision-Centric VQA Benchmark</h2>
          <div class="content has-text-justified">

            
            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              NaturalBench prevents blind solutions by including <b>two</b> questions and <b>two</b> images per sample with alternating answers. This ensures that blind models cannot succeed while giving the same answer regardless of the image or question. We use a semi-automated pipeline to collect NaturalBench:
            </p>
            <div class="item">
              <!-- Your image here -->
              <div style="display: flex; justify-content: center; align-items: center;">
                <img src="images/collection.jpg" alt="Image illustrating NaturalBench collection">
            </div>
            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              Given a natural image-text dataset like Flickr30K, (1) we first identify confounding image-text pairs that fail VLMs like CLIP, such as when CLIP mismatches an image with another image’s caption. (2) Next, we prompt ChatGPT to generate questions with different answers for each image, using the provided captions. Finally, human annotators filter out incorrect VQA samples. Unlike previous adversarial benchmarks, NaturalBench does not perturb images or questions but produces <b>natural adversarial samples</b>—questions about natural images that are challenging for VLMs but easy for humans. Below are some examples from NaturalBench, comparing the ground-truth answers with predictions from leading VLMs like GPT-4o, Qwen2-VL, Llama3.2-Vision, and Molmo:
            </p>
            <div class="item">
              <!-- Your image here -->
              <div style="display: flex; justify-content: center; align-items: center;">
                <img src="images/natural_teaser.jpg" alt="Image illustrating NaturalBench">
            </div>
            
            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              As shown, while humans easily solve these simple questions about natural images (with over 90% accuracy), state-of-the-art models, including the closed-source GPT-4o, still struggle.
            </p>

              
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Method Overview -->


  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">NaturalBench challenges leading VLMs</h2>
          <div class="content has-text-justified">

          <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
            We now show that NaturalBench poses a significant challenge to 55 state-of-the-art VLMs. To better understand model performance, beyond binary VQA accuracy (<b>Acc</b>), we introduce three metrics based on NaturalBench's paired image-question format.

            <ul> <li><b>Question Accuracy (Q-Acc):</b> Awards a point only if a model correctly answers a question for both images.</li> <li><b>Image Accuracy (I-Acc):</b> Awards a point when a model correctly answers both questions for an image.</li> <li><b>Group Accuracy (G-Acc):</b> Awards a point only when a model correctly answers all four (image, question) pairs in a test sample.</li> </ul>
            Below, we present the performance of VLMs across these metrics. We also highlight the performance gap (in terms of G-Acc) compared to humans in <span style="color: red;">red</span>:
          </p>
            </div>
            
            <div style="display: flex; justify-content: center; align-items: center;">
              <img src="images/performance.png" alt="Image illustrating Performance">
          </div>

          <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
            As shown in the table, all models fall well short of human performance. Even the latest and strongest models, such as BLIP-3 (XGen-MM), Cambrian-1, LLaVA-OneVision, Llama3.2-Vision, Molmo, and Qwen2-VL, lag 55% to 70% behind humans. The best closed-source model, GPT-4o, is still 52% behind.
          </p>

            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Why is NaturalBench challenging?</h2>
          <div class="content has-text-justified">
            <div class="item">
              <!-- Your image here -->

              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;"></p>
              <b>Compositionality:</b> Solving a NaturalBench sample always requires a combination of visio-linguitic reasoning skills, including object recognition, attribute binding, relation understanding, and advanced reasoning such as logic, comparison, differentiation (instance discrimination), counting, and world knowledge. We tag each (image, question) pair with all associated skills for a fine-grained analysis. In the paper, we show that even the best VLMs like GPT-4o still struggle with skills such as spatial orientation.
                </p>
              <div style="display: flex; justify-content: center; align-items: center;">
                <img src="images/tag_new.png" alt="Image illustrating Tags">
            </div>
  
            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
            <b>Biases:</b> NaturalBench exposes VLMs' biases towards certain answers like "Yes" and "B" regardless of the input image and question. We use the answer likelihood (<a href="https://linzhiqiu.github.io/papers/vqascore/"><span style="color: green;">VQAScore</span></a>) to perform a scoring-based evaluation by comparing the likelihood of correct (image, question, answer) triples over the incorrect ones to show that proper debiasing can double or triple the performance, even for GPT-4o. This suggests that NaturalBench can be a useful benchmark for evaluating methods that reduce biases (or hallucinations) in VLMs.
            </p>
            <div style="display: flex; justify-content: center; align-items: center;">
              <img src="images/debiasing.png" alt="Image illustrating Tags">
          </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Towards Dynamic Evaluation</h2>
          <div class="content has-text-justified">
            
            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              Since benchmarks often leak into foundation models' training data, it is crucial to update benchmarks using new data sources. Our benchmark curation method can be easily adapted to new image-text datasets. We expand NaturalBench by incorporating two recently proposed datasets: (1) <a href="https://arxiv.org/abs/2404.19753">DOCCI</a> with fine-grained captions over 100 words, and (2) <a href="https://google.github.io/crossmodal-3600/">XM3600</a> with captions in Chinese and Hindi. We hope our efforts will inspire future work on dynamic evaluation of VLMs.
            </p>
            </div>
            
            

            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Finetuning with NaturalBench</h2>
          <div class="content has-text-justified">
            
            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              Other than model evaluation, NaturalBench is also useful for model development. For example, we show in the paper that vision finetuning of GPT-4o and LLaVA using half of NaturalBench can significant boost their performance on the second half:
            </p>

            <div style="display: flex; justify-content: center; align-items: center;">
              <img src="images/NaturalBench_finetuned_g_acc.jpeg" alt="Image illustrating Performance Boost of Vision Finetuning">
          </div>

          <!-- Given the promising results, we plan to scale up NaturalBench for model post-training in future work. -->
            </div>


            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">GPT-o3 Results (2025 April)</h2>
          <div class="content has-text-justified">
  
            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              As of April 2025, GPT-o3 (with visual reasoning) achieves the highest performance on NaturalBench with a G-Acc of 51%.
            </p>
            <div style="display: flex; justify-content: center; align-items: center;">
              <img src="images/NaturalBench_bar.jpeg" alt="Image illustrating GPT-o3 Results">
            </div>

            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              We show that GPT-o3 can perform a smart "zoom‑in" visual chain‑of‑thought that allows it to crack questions that still stump GPT‑4o.
            </p>
            <div style="display: flex; justify-content: center; align-items: center;">
              <img src="images/gpt_4o_vs_gpt_o3.png" alt="Image illustrating GPT-o3 vs GPT-4o Results">
            </div>

            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              However, visual reasoning is not solved yet. We show that GPT-o3 can still make simple mistakes even after a full minute of reasoning--hallucinating something being moved while nothing is happening.
            </p>
            <div style="display: flex; justify-content: center; align-items: center;">
              <img src="images/gpt_o3_failure.png" alt="Image illustrating GPT-o3 Failures">
            </div>

            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              We hope to see stronger visual reasoning models on NaturalBench in the future.
            </p>
          </div>
  
  
  
  
        </div>
      </div>
    </div>
    </div>
  </section>

  <!--BibTex citation -->
  <section class="section hero is-light2" id="BibTeX (TO UPDATE)">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{naturalbench,
        title={NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples},
        author={Li, Baiqi and Lin, Zhiqiu and Peng, Wenxuan and Nyandwi, Jean de Dieu and Jiang, Daniel and Ma, Zixian and Khanuja, Simran and Krishna, Ranjay and Neubig, Graham and Ramanan, Deva},
        booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
        year={2024},
        }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

</body>

</html>
